{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, ComplementNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_attack_types(filename):\n",
    "    \"\"\"\n",
    "    Generate a mapping that looks like:\n",
    "\n",
    "    {\n",
    "        'teardrop': {\n",
    "            'encoding': 0,\n",
    "            'category': 'dos'\n",
    "        },\n",
    "        'smurf': {\n",
    "            'encoding': 1,\n",
    "            'category': 'dos'\n",
    "        },\n",
    "        ...\n",
    "    }\n",
    "\n",
    "    The 'encoding' becomes important in some learning algorithms. We have to encode text\n",
    "    into numbers so some algorithms can process them.\n",
    "    \"\"\"\n",
    "    attack_map = {}\n",
    "    attack_encoding = {}\n",
    "    count = 0\n",
    "    with open(filename) as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        attack, category = line.split()\n",
    "        if attack not in attack_map:\n",
    "            attack_map[attack] = {\n",
    "                'encoding': count,\n",
    "                'category': category\n",
    "            }\n",
    "            count += 1\n",
    "    return attack_map\n",
    "\n",
    "\n",
    "def encode_data(train_data, cols):\n",
    "    \"\"\"\n",
    "    Encode any strings in the training data so that they are integers.\n",
    "    Also return the map of encodings.\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\" Fulbert -- Change the funciton a bit, so cols is a parameter \"\"\"\n",
    "    encodings = {}\n",
    "    for col in cols:\n",
    "        unique_values = train_data[col].unique()\n",
    "        mapping = {}\n",
    "        reverse_mapping = {}  # Used for lookup later if we need it\n",
    "        for j, value in enumerate(unique_values):\n",
    "            mapping[value] = j\n",
    "            reverse_mapping[j] = value\n",
    "        # Encode strings like ('tcp', 'udp', 'icmp') into (0, 1, 2)\n",
    "        train_data[col] = train_data[col].map(mapping)\n",
    "        encodings[col] = reverse_mapping\n",
    "    return encodings\n",
    "\n",
    "\n",
    "def parse_data(filename):\n",
    "    return pd.read_csv(filename, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Running project')\n",
    "attack_map = parse_attack_types(r'C:\\Users\\CODER\\CTDUML\\CTDUML\\.venv\\CyberThreatDetection\\Dataset/attack_types.txt')\n",
    "print('Attack mapping:')\n",
    "print(attack_map)\n",
    "train_data = parse_data(r'C:\\Users\\CODER\\CTDUML\\CTDUML\\.venv\\CyberThreatDetection\\Dataset/Dataset.data_10_percent')\n",
    "print('Raw data:')\n",
    "print(train_data[:2])\n",
    "## See labeled data distribution\n",
    "encodings = encode_data(train_data, (1, 2, 3))\n",
    "print('Encoded data:')\n",
    "print(train_data[:2])\n",
    "print('Encodings:')\n",
    "print(encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot frequency of each labels\n",
    "train_data[41].value_counts().plot(kind='bar')\n",
    "train_data[41].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Describe Data using Panda Describe for 10%''' \n",
    "train_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Try different attack mapping\n",
    "        normal = 0  \n",
    "        dos = 1\n",
    "        probe = 2 \n",
    "        r2l = 3 \n",
    "        u2r = 4 \n",
    "        \n",
    "        We will end up with 5 classes instead of 23 classes\n",
    "        \n",
    "        One thing to note when mapping the 5 classes, there is a period at the end of the \n",
    "        label on the original file!!! Don't forget about that. \n",
    "\"\"\"\n",
    "\n",
    "def revised_attack_mapping(attack_map):\n",
    "    revised_attack_map = {}\n",
    "    for name, value in attack_map.items():\n",
    "        revised_attack_map[name + \".\"] = (value['category']) #Don't forget about the period at the end!!\n",
    "    return revised_attack_map\n",
    "\n",
    "def attack_category_encoding():\n",
    "    \"\"\"\n",
    "        normal = 0  \n",
    "        dos = 1\n",
    "        probe = 2 \n",
    "        r2l = 3 \n",
    "        u2r = 4 \n",
    "    \"\"\"    \n",
    "    attack_category_map = {}\n",
    "    attack_category_map ['normal'] = 0\n",
    "    attack_category_map ['dos'] = 1\n",
    "    attack_category_map ['probe'] = 2\n",
    "    attack_category_map ['r2l'] = 3\n",
    "    attack_category_map ['u2r'] = 4\n",
    "    return attack_category_map\n",
    "\n",
    "category_attack_map = revised_attack_mapping(attack_map)\n",
    "category_attack_map['normal.'] = \"normal\" #Removing the period at the end!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_attack_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_category_map = attack_category_encoding()\n",
    "attack_category_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Map 10% Dataset\n",
    "train_data[41] = train_data[41].map(category_attack_map)\n",
    "train_data[41].value_counts().plot(kind='bar')\n",
    "train_data[41].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data[41].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Method to test sklearn classifier\n",
    "def test_classifier(clf):\n",
    "    start = time.time()\n",
    "    clf = clf.fit(X_train, y_train)\n",
    "    training_ends = time.time()\n",
    "    prediction = clf.predict(X_test)\n",
    "    prediction_ends = time.time()\n",
    "    result = (metrics.classification_report(y_test, prediction, output_dict = True))\n",
    "    training_time = training_ends - start\n",
    "    testing_time = prediction_ends - training_ends\n",
    "    print (metrics.classification_report(y_test, prediction))\n",
    "    acc = metrics.accuracy_score(y_test, prediction)\n",
    "    print (\"Accuracy Score: %s\" % acc)\n",
    "    print (\"Classifier Training time = %s\" % training_time)\n",
    "    print (\"Classifier Prediction time = %s\" % testing_time)\n",
    "    train_time.append(training_time)\n",
    "    test_time.append(testing_time)\n",
    "    accuracy.append(acc)\n",
    "    return clf, result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split train and test set \n",
    "X = train_data.drop(columns=[41])\n",
    "y = train_data[[41]]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=15)\n",
    "algorithm= []\n",
    "train_time = []\n",
    "test_time = []\n",
    "accuracy = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Multinomial Naive Bayes Classifier\n",
    "clf_MultinomialNB = MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
    "clf_MultinomialNB, mreport= test_classifier(clf_MultinomialNB)\n",
    "algorithm.append(\"MultinomialNB\")\n",
    "## Bernoulli Naive Bayes Classifier\n",
    "clf_BernoulliNB = BernoulliNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
    "clf_BernoulliNB, breport = test_classifier(clf_BernoulliNB)\n",
    "algorithm.append(\"BernoulliNB\")\n",
    "## Complement Naive Bayes Classifier\n",
    "clf_ComplementNB = ComplementNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
    "clf_ComplementNB, creport = test_classifier(clf_ComplementNB)\n",
    "algorithm.append(\"ComplementNB\")\n",
    "## Linear SVC Classifier\n",
    "clf_LinearSVC = LinearSVC(random_state=0, tol=1e-5)\n",
    "clf_LinearSVC, lreport= test_classifier(clf_LinearSVC)\n",
    "algorithm.append(\"LinearSVC\")\n",
    "## Decision Tree Classifier\n",
    "clf_DecisionTree = DecisionTreeClassifier()\n",
    "clf_LinearSVC, treereport = test_classifier(clf_DecisionTree)\n",
    "algorithm.append(\"DecisionTree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    normal = 0  \n",
    "    dos = 1\n",
    "    probe = 2 \n",
    "    r2l = 3 \n",
    "    u2r = 4 \n",
    "\"\"\"    \n",
    "\n",
    "def extract_algorithm_result (report):\n",
    "    result = []\n",
    "    getresult_inlist(report, 'normal', result)\n",
    "    getresult_inlist(report, 'dos', result)\n",
    "    getresult_inlist(report, 'probe', result)\n",
    "    getresult_inlist(report, 'r2l', result)\n",
    "    getresult_inlist(report, 'u2r', result)\n",
    "    return result\n",
    "def getresult_inlist (report, label, resultlist):\n",
    "    for index in report[label]:\n",
    "        resultlist.append(report[label][index])\n",
    "         # Precision, recall, f1-score, support\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_algorithm_result(report, title):\n",
    "    #indexes = [0,1,2,4,5,6,8,9,10,12,13,14,16,17,18]\n",
    "    result = extract_algorithm_result(report)\n",
    "    ## Plot Precision\n",
    "    indexes = [0,4,8,12,16]\n",
    "    algorithm_title = (\"Precision %s\" % title)\n",
    "    plot_metric(result, indexes, algorithm_title)\n",
    "        \n",
    "    ## Plot Recall\n",
    "    indexes = [1,5,9,13,17]\n",
    "    algorithm_title = (\"Recall %s\" % title)\n",
    "    plot_metric(result, indexes, algorithm_title)\n",
    "    \n",
    "    ## Plot f1-score\n",
    "    indexes = [2,6,10,14,18]\n",
    "    algorithm_title = (\"F1-Score of %s\" % title)\n",
    "    plot_metric(result, indexes, algorithm_title)\n",
    "        \n",
    "    ## Plot Support\n",
    "    indexes = [3,7,11,15,19]\n",
    "    algorithm_title = (\"Support %s\" % title)\n",
    "    plot_support(result, indexes, algorithm_title) # Note this one is a different function\n",
    "\n",
    "def plot_metric(result, indexes, algorithm_title):\n",
    "    fig, ax = plt.subplots()\n",
    "    Yval = [result[x] * 100 for x in indexes]\n",
    "    Xval = np.linspace(1,7,5)\n",
    "    barlist = plt.bar(Xval,Yval)\n",
    "    barlist[0].set_color('g')\n",
    "    barlist[1].set_color('r')\n",
    "    barlist[2].set_color('magenta')\n",
    "    barlist[3].set_color('yellow')\n",
    "    barlist[4].set_color('darkblue')\n",
    "    plt.suptitle(algorithm_title)\n",
    "    plt.legend((barlist[0], barlist[1], barlist[2], barlist[3], barlist[4]),('normal', 'dos', 'probe', 'r2l', 'u2r' ), loc='best')\n",
    "\n",
    "def plot_support(result, indexes, algorithm_title):\n",
    "    fig, ax = plt.subplots()\n",
    "    Yval = [result[x] for x in indexes]\n",
    "    Xval = np.linspace(1,7,5)\n",
    "    barlist = plt.bar(Xval,Yval)\n",
    "    barlist[0].set_color('g')\n",
    "    barlist[1].set_color('r')\n",
    "    barlist[2].set_color('magenta')\n",
    "    barlist[3].set_color('yellow')\n",
    "    barlist[4].set_color('darkblue')\n",
    "    plt.suptitle(algorithm_title)\n",
    "    plt.legend((barlist[0], barlist[1], barlist[2], barlist[3], barlist[4]),('normal', 'dos', 'probe', 'r2l', 'u2r' ), loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot MultinomialNB Result \n",
    "plot_algorithm_result(mreport, \"MultinomialNB on 10% datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot BernoulliNB Result \n",
    "plot_algorithm_result(breport, \"BernoulliNB on 10% datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot ComplementNB Result \n",
    "plot_algorithm_result(creport, \"ComplementNB on 10% datasets\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot LinearSVC Result \n",
    "plot_algorithm_result(lreport, \"LinearSVC on 10% datasets\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot DecisionTree Result \n",
    "plot_algorithm_result(treereport, \"DecisionTree on 10% datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load full dataset. Plot and print frequency of each class in the full dataset\n",
    "train_data_2 = parse_data(r'C:\\Users\\CODER\\CTDUML\\CTDUML\\.venv\\CyberThreatDetection\\Dataset\\kddcup.data.corrected')\n",
    "train_data_2.head()\n",
    "encodings = encode_data(train_data_2, (1, 2, 3))\n",
    "train_data_2[41].value_counts().plot(kind='bar')\n",
    "train_data_2[41].value_counts()\n",
    "algorithm_10 = algorithm\n",
    "train_time_10 = train_time\n",
    "test_time_10 = test_time\n",
    "accuracy_10 = accuracy\n",
    "algorithm= []\n",
    "train_time = []\n",
    "test_time = []\n",
    "accuracy = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot and print frequency of each class after mapping\n",
    "train_data_2[41] = train_data_2[41].map(category_attack_map)\n",
    "train_data_2[41].value_counts().plot(kind='bar')\n",
    "train_data_2[41].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data_2[41] = train_data_2[41].map(attack_category_encoding())\n",
    "#train_data_2[41].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Describe Data using Panda Describe for 100%''' \n",
    "train_data_2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_data_2.drop(columns=[41])\n",
    "y = train_data_2[[41]]\n",
    "#X = X.drop(columns=[19])\n",
    "#X = X.drop(columns=[20])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bernoulli Naive Bayes Classifier\n",
    "clf_BernoulliNB = BernoulliNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
    "clf_BernoulliNB, breport= test_classifier(clf_BernoulliNB)\n",
    "plot_algorithm_result(breport, \"BernoulliNB on 100% datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Decision Tree Classifier\n",
    "clf_DecisionTree = DecisionTreeClassifier()\n",
    "clf_DecisionTree, treereport = test_classifier(clf_DecisionTree)\n",
    "plot_algorithm_result(treereport, \"DecisionTree on 100% datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_classifier(clf):\n",
    "    start = time.time()\n",
    "    clf = clf.fit(X_train, y_train)\n",
    "    training_ends = time.time()\n",
    "    prediction = clf.predict(X_test)\n",
    "    prediction_ends = time.time()\n",
    "    result = (metrics.classification_report(y_test, prediction, output_dict = True))\n",
    "    training_time = training_ends - start\n",
    "    testing_time = prediction_ends - training_ends\n",
    "    print (metrics.classification_report(y_test, prediction))\n",
    "    acc = metrics.accuracy_score(y_test, prediction)\n",
    "    print (\"Accuracy Score: %s\" % acc)\n",
    "    print (\"Classifier Training time = %s\" % training_time)\n",
    "    print (\"Classifier Prediction time = %s\" % testing_time)\n",
    "    return clf, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Decision Tree Classifier Parameter tuning\n",
    "clf_DecisionTree = DecisionTreeClassifier(criterion = 'entropy')\n",
    "clf_DecisionTree = test_classifier(clf_DecisionTree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_DecisionTree = DecisionTreeClassifier(criterion = 'entropy', class_weight = 'balanced')\n",
    "clf_DecisionTree = test_classifier(clf_DecisionTree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight={}\n",
    "weight['dos'] = 1\n",
    "weight['normal'] = 1\n",
    "weight['probe'] = 1\n",
    "weight['r2l'] = 1\n",
    "tree_report_entropy = []\n",
    "for classweight in range (1, 202, 25):\n",
    "    weight['u2r'] = classweight\n",
    "    clf_DecisionTree = DecisionTreeClassifier(criterion = 'entropy', class_weight = weight)\n",
    "    clf_DecisionTree, report = tune_classifier(clf_DecisionTree)\n",
    "    tree_report_entropy.append(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_parameter_tuning(report, title):\n",
    "    fig, ax = plt.subplots()\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "    f1_score_list = []    \n",
    "    for i in range (0,9):\n",
    "        precision_list.append(tree_report_entropy[i]['u2r']['precision'])\n",
    "        recall_list.append(tree_report_entropy[i]['u2r']['recall'])\n",
    "        f1_score_list.append(tree_report_entropy[i]['u2r']['f1-score'])\n",
    "    X = list(range(1, 202, 25))\n",
    "    plt.plot(X, precision_list, label = \"Precision\")\n",
    "    plt.plot(X, recall_list, label = \"Recall\")\n",
    "    plt.plot(X, f1_score_list, label = \"f1_score_list\")\n",
    "    plt.xlabel(\"u2r Weight\")\n",
    "    plt.legend(loc = \"best\")\n",
    "    plt.suptitle(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight={}\n",
    "weight['dos'] = 1\n",
    "weight['normal'] = 1\n",
    "weight['probe'] = 1\n",
    "weight['r2l'] = 1\n",
    "tree_report = []\n",
    "for classweight in range (1, 202, 25):\n",
    "    weight['u2r'] = classweight\n",
    "    clf_DecisionTree = DecisionTreeClassifier(class_weight = weight)\n",
    "    clf_DecisionTree, report = tune_classifier(clf_DecisionTree)\n",
    "    tree_report.append(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_parameter_tuning(report, title):\n",
    "    fig, ax = plt.subplots()\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "    f1_score_list = []    \n",
    "    for i in range (0,9):\n",
    "        precision_list.append(tree_report_entropy[i]['u2r']['precision'])\n",
    "        recall_list.append(tree_report_entropy[i]['u2r']['recall'])\n",
    "        f1_score_list.append(tree_report_entropy[i]['u2r']['f1-score'])\n",
    "    X = list(range(1, 202, 25))\n",
    "    plt.plot(X, precision_list, label = \"Precision\")\n",
    "    plt.plot(X, recall_list, label = \"Recall\")\n",
    "    plt.plot(X, f1_score_list, label = \"f1_score_list\")\n",
    "    plt.xlabel(\"u2r Weight\")\n",
    "    plt.legend(loc = \"best\")\n",
    "    plt.suptitle(title)\n",
    "plot_parameter_tuning(tree_report_entropy, 'Decision Tree Parameter Tuning (Entropy Criteria)')\n",
    "plot_parameter_tuning(tree_report, 'Decision Tree Parameter Tuning (GINI Criteria)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Final Decision Tree Classifier\n",
    "weight={}\n",
    "weight['dos'] = 1\n",
    "weight['normal'] = 1\n",
    "weight['probe'] = 1\n",
    "weight['r2l'] = 1\n",
    "weight['u2r'] = 102\n",
    "clf_DecisionTree_final = DecisionTreeClassifier(criterion = 'entropy', class_weight = weight)\n",
    "clf_DecisionTree_final, treereport_final = test_classifier(clf_DecisionTree_final)\n",
    "## Plot Final DecisionTree Result \n",
    "plot_algorithm_result(treereport_final, \"Final DecisionTree on 100% datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving Model\n",
    "import pickle\n",
    "pickle.dump(clf_DecisionTree_final, open(\"DT_model.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
